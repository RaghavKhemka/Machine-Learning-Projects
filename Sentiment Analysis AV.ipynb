{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment AV.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1kzXVO4FlY9B1xK06AX132WtLqhEfohsR","authorship_tag":"ABX9TyM6x9vrKJV17EB0YmSW2eSP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"geF1ITpZiQsA","executionInfo":{"status":"ok","timestamp":1601545546694,"user_tz":-330,"elapsed":13155,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"600ddc97-8e2c-4474-b8c1-097cb3af2d9c","colab":{"base_uri":"https://localhost:8080/","height":590}},"source":["!pip install transformers\n","from transformers import TFBertModel, BertTokenizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 16.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 29.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 43.1MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=2ae4004041ad61753bb21c7f0fa926f55981fe07d252dd32187ab0819362d7bf\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iyWrzQVMZQkW"},"source":["import csv\n","import string\n","import re\n","text = []\n","labels = []\n","inputfile = csv.reader(open('./train_2kmZucJ.csv','r'))\n","for row in inputfile:\n","  text.append(row[2])\n","  labels.append(row[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hk92w-QeaZop"},"source":["text = text[1:]\n","temp = text\n","text = []\n","for i in temp:\n","  text.append(i.lower())\n","labels = labels[1:]\n","labelss = []\n","for i in labels:\n","  i = int(i)\n","  labelss.append(i)\n","labels = labelss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajeHP5M1aqLM"},"source":["def preprocess(input_text):\n","  \n","  ascii = set(string.printable)\n","  sentences = []\n","  for sentence in input_text:\n","    words = []\n","    sentence = sentence.split(' ')\n","    x = ['@','%','&']\n","    for word in sentence:\n","      c = 0\n","      for i in x:\n","        c += word.count(i)\n","      if c>0:\n","        continue\n","      word = word.replace('#','')\n","      if word[:4] == 'http':\n","        word = 'http_link'   \n","      exclude = ['-',':','.','?',')','(','^'] \n","      word = ''.join(ch for ch in word if ch not in exclude)\n","      if len(word)> 15:\n","        continue\n","      for l in word.split():\n","        if l not in ascii:\n","          continue\n","      # word = ''.join(ch for ch in word if ch in ascii)\n","      words.append(word)\n","    words = ' '.join(words)\n","    words = re.sub(\"\\d+\", \"\", words)\n","    sentences.append(words.lower())\n","  return(sentences)\n","text_new = preprocess(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TesGF_812B9o","executionInfo":{"status":"ok","timestamp":1601550282814,"user_tz":-330,"elapsed":1084,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"d1ca6e7e-3a5d-4a5f-bbac-d525570cf491","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["tmp = 0\n","lens = []\n","for i in text_new:\n","  tmp += len(i.split())\n","  lens.append(len(i.split()))\n","print(tmp/len(text_new))\n","print(lens[:20])\n","print(text_new[:20])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15.915277777777778\n","[13, 15, 15, 17, 23, 11, 10, 27, 13, 14, 25, 19, 16, 19, 16, 12, 15, 11, 14, 22]\n","['fingerprint pregnancy test http_link android apps beautiful cute health igers iphoneonly iphonesia iphone', 'finally a transparant silicon case  thanks to my uncle  yay sony xperia s sonyexperias… http_link', 'we love this! would you go talk makememories unplug relax iphone smartphone wifi connect http_link', \"i'm wired i know i'm george i was made that way ; iphone cute daventry home http_link\", \"what amazing service! apple won't even talk to me about a question i have unless i pay them $ for their stupid support!\", 'iphone software update fucked up my phone big time stupid iphones', 'happy for us  instapic instadaily us sony xperia xperiaz http_link', 'new type c charger cable uk http_link … bay amazon etsy new year rob cross toby young evemun mcmafia taylor spectre  newyear starting  recipes technology samsunggalaxys iphonex', 'bout to go shopping again listening to music iphone justme music likeforlike http_link', 'photo fun selfie pool water sony camera picoftheday sun instagood boy cute outdoor http_link', 'hey apple when you make a new ipod dont make it a new color or inches thinner make it not crash every five fuckin minite', 'ha! not heavy machinery but it does what i need it to really dropped the ball with that design', 'contemplating giving in to the iphone bandwagon simply because cellcom has no new androids depressing idontwantto', \"i just made another crazy purchase lol my theory is 'work hard, play hard' lol ipad apple shopping http_link\", 'the battery is so painful! i charge it overnight and by lunchtime battery is dead! hateorange', 'from deepellum towards downtown dallas bigd saturday rxm sony summer urban http_link', 'like and share if you want this d phone case for iphone iphone s  http_link', 'go crazy !! iphonesia, iphone, instagood, instagram, photooftheday, tweegram, ,… http_link', \"the reason i don't have one iphone suck apple truth truthbetold agree fact realitycheck\", \"how is the apple store gunna be out of c screens ! it's monday and ur the fucking apple store !!! fuckingpissed\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"klm-96CEsu1R"},"source":["from sklearn.model_selection import train_test_split\n","train_sents,test_sents, train_labels, test_labels  = train_test_split(text_new,labels,test_size=0.00001)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHqlbFkF49_S","executionInfo":{"status":"ok","timestamp":1601550295740,"user_tz":-330,"elapsed":4508,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"cdd449cf-f3b7-414e-fc73-8b7e25b52e93","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n","def encoder(sentences):\n","  ids = []\n","  for item in sentences:\n","    encoding = tokenizer.encode_plus(\n","    item,\n","    max_length=26,\n","    truncation = True,\n","    add_special_tokens=True,\n","    return_token_type_ids=False,\n","    pad_to_max_length=True,\n","    return_attention_mask=False)\n","    ids.append(encoding['input_ids'])\n","  return ids\n","train_ids = encoder(train_sents)\n","test_ids = encoder(test_sents)  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_gkdjHohA2m5"},"source":["import tensorflow as tf\n","train_ids = tf.convert_to_tensor(train_ids)\n","test_ids = tf.convert_to_tensor(test_ids)\n","test_labels = tf.convert_to_tensor(test_labels)\n","train_labels = tf.convert_to_tensor(train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTiMoLYhC68X","executionInfo":{"status":"ok","timestamp":1601551095363,"user_tz":-330,"elapsed":3952,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"472529d1-0155-4a69-a604-738d381337af","colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["def build_model():\n","    bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\n","    input_word_ids = tf.keras.Input(shape=(26,), dtype=tf.int32, name=\"input_word_ids\")\n","    #input_mask = tf.keras.Input(shape=(26,), dtype=tf.int32, name=\"input_mask\")    \n","    embedding = bert_encoder([input_word_ids])\n","    print(type(embedding[0]))\n","    dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n","    dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n","    dense = tf.keras.layers.Dropout(0.1)(dense)   \n","    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n","    model = tf.keras.Model(inputs=[input_word_ids], outputs=output)  \n","    return model\n","model = build_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["<class 'tensorflow.python.framework.ops.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E9BU1GD3EdDp","executionInfo":{"status":"ok","timestamp":1601551432444,"user_tz":-330,"elapsed":1170,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"690b0a04-771d-4edf-8894-7448208ed20a","colab":{"base_uri":"https://localhost:8080/","height":342}},"source":["model.compile(tf.keras.optimizers.Adam(3e-6), loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_25\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_word_ids (InputLayer)  [(None, 26)]              0         \n","_________________________________________________________________\n","tf_bert_model_12 (TFBertMode ((None, 26, 768), (None,  109482240 \n","_________________________________________________________________\n","lambda_12 (Lambda)           (None, 768)               0         \n","_________________________________________________________________\n","dense_24 (Dense)             (None, 128)               98432     \n","_________________________________________________________________\n","dropout_493 (Dropout)        (None, 128)               0         \n","_________________________________________________________________\n","dense_25 (Dense)             (None, 1)                 129       \n","=================================================================\n","Total params: 109,580,801\n","Trainable params: 109,580,801\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KGZeRe47Doop","executionInfo":{"status":"ok","timestamp":1601551494971,"user_tz":-330,"elapsed":60885,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"35dc681f-8e78-4f65-8b05-7a2315598abc","colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["history = model.fit(x = train_ids, y = train_labels, epochs = 1, verbose = 1, batch_size = 16, validation_data = (test_ids, test_labels))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n","495/495 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9583WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb4ec778ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","495/495 [==============================] - 45s 91ms/step - loss: 0.1172 - accuracy: 0.9583 - val_loss: 0.0562 - val_accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"skl4oN6vjnbo"},"source":["import pandas as pd\n","ds = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AV/test_oJQbWVk.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZ-Mye0Enmqv","executionInfo":{"status":"ok","timestamp":1601550457383,"user_tz":-330,"elapsed":2159,"user":{"displayName":"Raghav Khemka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhP97kyjau6VRuhCI5dJSmyS1ZR45vCNEhgrMt5hQ=s64","userId":"08481167791138987896"}},"outputId":"eca796f1-0f74-404d-f947-2b763929f88c","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["sub_ids = ds.id.values\n","sub_tweets = ds.tweet.values\n","sub_tweets = preprocess(sub_tweets)\n","sub_tweets = encoder(sub_tweets)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YUHgcs5MoUCe"},"source":["predicted = model.predict(sub_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ia0j90hMoxh8"},"source":["\n","predictions = [int(round(i[0])) for i in predicted]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NeKlEMb1o16k"},"source":["new_df = pd.DataFrame() \n","new_df['id'] = sub_ids\n","new_df['label'] = predictions\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkH2ZZbup8Q3"},"source":["new_df.to_csv('./submission10d.csv',index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMdzp8Nspzin"},"source":[""],"execution_count":null,"outputs":[]}]}